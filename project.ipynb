{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "import requests\n",
    "# import eventlet\n",
    "# eventlet.monkey_patch()\n",
    "import string\n",
    "\n",
    "import urllib.request\n",
    "from bs4.element import Comment\n",
    "\n",
    "from collections import Counter \n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "#word_tokenize accepts a string as an input, not a file. \n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import csv\n",
    "\n",
    "from glob import glob; from os.path import expanduser\n",
    "\n",
    "def write_to_webpage_csv():\n",
    "    conn = sqlite3.connect('test.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"select * from webpage;\")\n",
    "    with open(\"webpage.csv\", \"w\", newline='') as csv_file:  # Python 3 version    \n",
    "#     with open(\"out.csv\", \"wb\") as csv_file:              # Python 2 version\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow([i[0] for i in cursor.description]) # write headers\n",
    "        csv_writer.writerows(cursor)\n",
    "        \n",
    "def write_to_links_csv():\n",
    "    conn = sqlite3.connect('test.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"select * from link;\")\n",
    "    with open(\"link.csv\", \"w\", newline='') as csv_file:  # Python 3 version    \n",
    "#     with open(\"out.csv\", \"wb\") as csv_file:              # Python 2 version\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow([i[0] for i in cursor.description]) # write headers\n",
    "        csv_writer.writerows(cursor)\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "crawled = set ()\n",
    "timed_out = set ()\n",
    "error_urls = set ()\n",
    "error_urls.add('http://vpsc.uiowa.edu/index.html')\n",
    "links = []\n",
    "url_words = []\n",
    "\n",
    "def crawl_helper(url):\n",
    "    global crawled\n",
    "    crawled.add(url) \n",
    "    try:\n",
    "        if(url not in error_urls and not url.startswith('http://vpsc.uiowa.edu/') \n",
    "           and not url.startswith('http://events.uiowa.edu/')\n",
    "           and not url.startswith('https://ispo-apps.its.uiowa.edu/dports/')\n",
    "           and not (url.endswith('.pdf') or url.endswith('.docx') or url.endswith('.xlsx') or url.endswith('.pptx'))):\n",
    "            html = urllib.request.urlopen(url, timeout=15).read()\n",
    "            top_10 = top_10_words(html)\n",
    "            write_to_webpage(url, top_10)\n",
    "            soup = BeautifulSoup(html)\n",
    "            ahrefs =  soup.find_all('a', href=True)\n",
    "    except:\n",
    "        error_urls.add(url)\n",
    "        return\n",
    "\n",
    "    \n",
    "    for a in ahrefs:\n",
    "        # Filter out certain long timeout urls\n",
    "        if(a['href'] not in error_urls and a['href'].startswith('http')):\n",
    "            if('uiowa.edu' in a['href'] and (not a['href'].startswith('#')) \n",
    "               and (not a['href'].startswith('http://vpsc.uiowa.edu/')) \n",
    "               and (not a['href'].startswith('http://events.uiowa.edu/'))\n",
    "               and (not a['href'].startswith('https://ispo-apps.its.uiowa.edu/dports/'))\n",
    "               and not (a['href'].endswith('.pdf') or a['href'].endswith('.docx') or a['href'].endswith('.xlsx') or a['href'].endswith('.pptx'))):\n",
    "                print(\"Found the URL:\", a['href'])\n",
    "                links.append([url,a['href']])\n",
    "                write_to_link(url,a['href'])\n",
    "                if(a['href'] not in crawled): \n",
    "                    crawl_helper(a['href'])\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "                \n",
    "        \n",
    "def top_10_words(html):  \n",
    "    data_set = text_from_html(html)\n",
    "\n",
    "    # split() returns list of all the words in the string \n",
    "    split_it = data_set.split() \n",
    "    stemmed = []\n",
    "    words = []\n",
    "\n",
    "\n",
    "    ps = PorterStemmer() \n",
    "\n",
    "    for w in split_it: \n",
    "        stemmed.append(ps.stem(w)) \n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    for word in stemmed: \n",
    "        if not word in stop_words:\n",
    "            if not any(char in string.punctuation for char in word):\n",
    "                words.append(word)\n",
    "            else:\n",
    "                continue\n",
    "    # Pass the split_it list to instance of Counter class. \n",
    "    counter = Counter(words) \n",
    "\n",
    "    # most_common() produces k frequently encountered \n",
    "    # input values and their respective counts. \n",
    "    most_occur = counter.most_common(10) \n",
    "\n",
    "    print(most_occur)\n",
    "    return most_occur\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    crawl_helper(url)\n",
    "\n",
    "def main():\n",
    "#     drop_table_webpage()\n",
    "#     create_table_webpage()\n",
    "# #     drop_table_link()\n",
    "#     create_table_link()\n",
    "#     crawl('https://uiowa.edu')\n",
    "    write_to_webpage_csv()\n",
    "    write_to_links_csv()\n",
    "    \n",
    "def drop_table_webpage():    \n",
    "    conn = sqlite3.connect('test.db')\n",
    "    conn.execute(\"DROP TABLE WEBPAGE\")\n",
    "    print(\"Webpage table dropped successfully\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def drop_table_link():    \n",
    "    conn = sqlite3.connect('test.db')\n",
    "    conn.execute(\"DROP TABLE LINK\")\n",
    "    print(\"Link table dropped successfully\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def create_table_webpage():    \n",
    "    conn = sqlite3.connect('test.db')\n",
    "    conn.execute('''CREATE TABLE WEBPAGE\n",
    "             (ID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "             URL TEXT    NOT NULL,\n",
    "             WORD1            TEXT,\n",
    "             WORD2            TEXT,\n",
    "             WORD3            TEXT,\n",
    "             WORD4            TEXT,\n",
    "             WORD5            TEXT,\n",
    "             WORD6            TEXT,\n",
    "             WORD7            TEXT,\n",
    "             WORD8            TEXT,\n",
    "             WORD9            TEXT,\n",
    "             WORD10            TEXT);''')\n",
    "    print(\"Webpage table created successfully\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def create_table_link():\n",
    "    conn = sqlite3.connect('test.db')\n",
    "    conn.execute('''CREATE TABLE LINK\n",
    "             (ID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "             FROM_URL TEXT NOT NULL,\n",
    "             TO_URL TEXT NOT NULL);''')\n",
    "    print(\"Link table created successfully\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def write_to_webpage(url, words):\n",
    "    conn = sqlite3.connect('test.db')\n",
    "    word_array = []\n",
    "    for i in range (0,10):\n",
    "        if i < len(words)-1:\n",
    "            word_array.append(words[i][0])\n",
    "        else:\n",
    "            word_array.append(\"\")\n",
    "#     print(word1)\n",
    "    conn.execute(\"INSERT INTO WEBPAGE(URL, WORD1, WORD2, WORD3, WORD4, WORD5, WORD6, WORD7, WORD8, WORD9, WORD10) VALUES (?,?,?,?,?,?,?,?,?,?,?)\",(url, word_array[0], word_array[1], word_array[2], word_array[3], word_array[4], word_array[5], word_array[6], word_array[7], word_array[8], word_array[9]))\n",
    "    print(\"Webpage record created successfully\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def write_to_link(from_url, to_url):\n",
    "    conn = sqlite3.connect('test.db')\n",
    "    conn.execute(\"INSERT INTO LINK(FROM_URL, TO_URL) VALUES (?,?)\",(from_url, to_url))\n",
    "    print(\"Link record created successfully\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
